{
  "module_name": "02_LoRA_Design_for_Temporal_Blocks",
  "difficulty_level": "Hard",
  "rendering_note": "Math tokens appear as plain text (e.g., alpha_bar_t, sqrt(beta(t)), mu, sigma^2). Render them with LaTeX/KaTeX/MathJax in the frontend.",
  "sections": [
    {
      "section_id": "A",
      "section_title": "Conceptual Recall (MCQs)",
      "total_questions": 20,
      "marking_scheme": { "correct": 2.0, "incorrect": -0.5 },
      "questions": [
        {
          "id": "q1",
          "type": "mcq",
          "question_text": "LoRA adapts a weight W by adding:",
          "code_snippet": null,
          "options": [
            "A) Random noise",
            "B) A low-rank update A B scaled by alpha/r",
            "C) A diagonal mask",
            "D) A bias-only term"
          ],
          "correct_option": "B",
          "explanation": "LoRA inserts trainable low-rank factors scaled by alpha/r."
        },
        {
          "id": "q2",
          "type": "mcq",
          "question_text": "To target temporal attention, LoRA should wrap:",
          "code_snippet": null,
          "options": [
            "A) Only spatial convs",
            "B) Q/K/V/Out projections inside temporal self-attn",
            "C) VAE decoder",
            "D) Text encoder"
          ],
          "correct_option": "B",
          "explanation": "Temporal LoRA belongs on temporal attention projections."
        },
        {
          "id": "q3",
          "type": "mcq",
          "question_text": "Higher LoRA rank generally means:",
          "code_snippet": null,
          "options": [
            "A) Fewer parameters",
            "B) More capacity and VRAM use",
            "C) Smaller gradients",
            "D) Fixed compute"
          ],
          "correct_option": "B",
          "explanation": "Rank controls adapter capacity and parameter count."
        },
        {
          "id": "q4",
          "type": "mcq",
          "question_text": "Zero-init of LoRA B (or the output) is helpful because:",
          "code_snippet": null,
          "options": [
            "A) It disables training",
            "B) Model starts identical to base, then gradually learns updates",
            "C) It halves LR",
            "D) It removes EMA"
          ],
          "correct_option": "B",
          "explanation": "Zero-init prevents early drift from base weights."
        },
        {
          "id": "q5",
          "type": "mcq",
          "question_text": "Sharing LoRA across temporal layers trades off:",
          "code_snippet": null,
          "options": [
            "A) Capacity for parameter efficiency",
            "B) VRAM for FLOPs",
            "C) Rank for beta",
            "D) T for H"
          ],
          "correct_option": "A",
          "explanation": "Shared adapters reduce params but limit specificity."
        },
        {
          "id": "q6",
          "type": "mcq",
          "question_text": "Per-head vs fused-head LoRA:",
          "code_snippet": null,
          "options": [
            "A) Same capacity",
            "B) Per-head can capture head-specific patterns at higher param cost",
            "C) Fused is always better",
            "D) Per-head disables guidance"
          ],
          "correct_option": "B",
          "explanation": "Per-head adds capacity; fused is cheaper."
        },
        {
          "id": "q7",
          "type": "mcq",
          "question_text": "When applying LoRA to convs, a common pattern is to:",
          "code_snippet": null,
          "options": [
            "A) Ignore kernel size",
            "B) Reshape conv kernels to linear (out, in*k*k) then apply LoRA",
            "C) Only update bias",
            "D) Reduce stride"
          ],
          "correct_option": "B",
          "explanation": "Convs can be treated as linear on flattened kernels."
        },
        {
          "id": "q8",
          "type": "mcq",
          "question_text": "LoRA scaling alpha is used to:",
          "code_snippet": null,
          "options": [
            "A) Change beta schedule",
            "B) Adjust magnitude of low-rank update",
            "C) Set batch size",
            "D) Fix VAE scale"
          ],
          "correct_option": "B",
          "explanation": "Alpha modulates adapter influence."
        },
        {
          "id": "q9",
          "type": "mcq",
          "question_text": "If only temporal LoRA are trained, spatial artifacts typically arise when:",
          "code_snippet": null,
          "options": [
            "A) Rank too high",
            "B) Temporal Q/K updates indirectly destabilize spatial features via shared projections",
            "C) Batch size small",
            "D) Guidance weight low"
          ],
          "correct_option": "B",
          "explanation": "Temporal updates can leak into shared layers if not isolated."
        },
        {
          "id": "q10",
          "type": "mcq",
          "question_text": "Adapter dropout (dropping LoRA output) can:",
          "code_snippet": null,
          "options": [
            "A) Freeze model",
            "B) Regularize adapters, reducing overfit",
            "C) Change beta",
            "D) Stop EMA"
          ],
          "correct_option": "B",
          "explanation": "Dropout on adapter output improves robustness."
        },
        {
          "id": "q11",
          "type": "mcq",
          "question_text": "Merging LoRA into base weights for inference:",
          "code_snippet": null,
          "options": [
            "A) Increases runtime overhead",
            "B) Produces a single fused weight for deployment; cannot unmerge afterwards",
            "C) Leaves adapters active",
            "D) Changes beta"
          ],
          "correct_option": "B",
          "explanation": "Merging is destructive if you discard adapters."
        },
        {
          "id": "q12",
          "type": "mcq",
          "question_text": "Temporal LoRA placement priority should be:",
          "code_snippet": null,
          "options": [
            "A) Lowest-res only",
            "B) All temporal attention layers, with emphasis on mid/high-res for motion detail",
            "C) VAE encoder only",
            "D) Text encoder"
          ],
          "correct_option": "B",
          "explanation": "Cover temporal attn across scales for coherence."
        },
        {
          "id": "q13",
          "type": "mcq",
          "question_text": "LoRA rank selection depends on:",
          "code_snippet": null,
          "options": [
            "A) Tokenizer length",
            "B) Target motion complexity and VRAM budget",
            "C) Beta schedule",
            "D) VAE stride"
          ],
          "correct_option": "B",
          "explanation": "Higher motion complexity needs more capacity."
        },
        {
          "id": "q14",
          "type": "mcq",
          "question_text": "Scaling LoRA outputs by timestep (FiLM-style) can help because:",
          "code_snippet": null,
          "options": [
            "A) Early steps need less structure",
            "B) High-noise steps may benefit from stronger temporal cues than low-noise steps",
            "C) VAE scale changes",
            "D) It reduces rank"
          ],
          "correct_option": "B",
          "explanation": "Time-aware scaling balances influence across noise levels."
        },
        {
          "id": "q15",
          "type": "mcq",
          "question_text": "LoRA on output projection (o_proj) of temporal attention:",
          "code_snippet": null,
          "options": [
            "A) Never needed",
            "B) Helps adjust mixing of head outputs across time",
            "C) Breaks model",
            "D) Only for VAE"
          ],
          "correct_option": "B",
          "explanation": "o_proj aggregates heads; LoRA can alter temporal mixing."
        },
        {
          "id": "q16",
          "type": "mcq",
          "question_text": "LoRA merging should be skipped when:",
          "code_snippet": null,
          "options": [
            "A) You need to dynamically swap adapters per prompt/task",
            "B) You want smaller checkpoints",
            "C) You want fixed weights",
            "D) You use CFG"
          ],
          "correct_option": "A",
          "explanation": "Unmerged adapters allow runtime swapping."
        },
        {
          "id": "q17",
          "type": "mcq",
          "question_text": "Per-layer alpha scaling allows:",
          "code_snippet": null,
          "options": [
            "A) Different influence per temporal block",
            "B) Changing beta",
            "C) VAE merge",
            "D) Token truncation"
          ],
          "correct_option": "A",
          "explanation": "Layer-specific scaling tunes impact."
        },
        {
          "id": "q18",
          "type": "mcq",
          "question_text": "If LoRA updates explode, first remedies include:",
          "code_snippet": null,
          "options": [
            "A) Increase LR",
            "B) Lower LR, clip grads, reduce rank/alpha",
            "C) Remove EMA",
            "D) Ignore"
          ],
          "correct_option": "B",
          "explanation": "Stabilize by reducing update magnitude."
        },
        {
          "id": "q19",
          "type": "mcq",
          "question_text": "Training temporal LoRA with batch T=1 usually:",
          "code_snippet": null,
          "options": [
            "A) Teaches motion",
            "B) Fails to learn temporal patterns; degenerate to spatial",
            "C) Improves VRAM",
            "D) Sets beta to zero"
          ],
          "correct_option": "B",
          "explanation": "Need multiple frames to learn temporal relations."
        },
        {
          "id": "q20",
          "type": "mcq",
          "question_text": "LoRA adapter dropout is typically applied to:",
          "code_snippet": null,
          "options": [
            "A) Inputs",
            "B) LoRA delta before adding to base weight",
            "C) Optimizer",
            "D) VAE decoder"
          ],
          "correct_option": "B",
          "explanation": "Drop adapter output to regularize."
        }
      ]
    },
    {
      "section_id": "B",
      "section_title": "Logic & Implementation Puzzles",
      "total_questions": 20,
      "marking_scheme": { "correct": 2.0, "incorrect": -0.5 },
      "questions": [
        {
          "id": "q21",
          "type": "parsons_problem",
          "question_text": "Arrange LoRA forward for linear layer.",
          "scrambled_code_blocks": [
            { "id": "b1", "text": "delta = self.B(self.A(x)) * (self.alpha / self.r)" },
            { "id": "b2", "text": "base = F.linear(x, self.weight, self.bias)" },
            { "id": "b3", "text": "return base + delta" },
            { "id": "b4", "text": "def forward(self, x):" }
          ],
          "correct_order": ["b4", "b2", "b1", "b3"],
          "explanation": "Compute base output then add scaled low-rank delta."
        },
        {
          "id": "q22",
          "type": "parsons_problem",
          "question_text": "Arrange to wrap temporal QKV with LoRA.",
          "scrambled_code_blocks": [
            { "id": "b1", "text": "self.q_lora = LoRA(self.to_q, r, alpha)" },
            { "id": "b2", "text": "def __init__(self, to_q, to_k, to_v, r, alpha):" },
            { "id": "b3", "text": "self.k_lora = LoRA(self.to_k, r, alpha)" },
            { "id": "b4", "text": "self.v_lora = LoRA(self.to_v, r, alpha)" }
          ],
          "correct_order": ["b2", "b1", "b3", "b4"],
          "explanation": "Wrap projections with LoRA modules."
        },
        {
          "id": "q23",
          "type": "parsons_problem",
          "question_text": "Arrange per-head LoRA split.",
          "scrambled_code_blocks": [
            { "id": "b1", "text": "heads = weight.view(h, d_head, in_dim)" },
            { "id": "b2", "text": "lora_heads = nn.ModuleList([LoRAHead(d_head, in_dim, r) for _ in range(h)])" },
            { "id": "b3", "text": "def build_per_head(weight, h, d_head, in_dim, r):" },
            { "id": "b4", "text": "return heads, lora_heads" }
          ],
          "correct_order": ["b3", "b1", "b2", "b4"],
          "explanation": "Split weight per head and attach LoRA heads."
        },
        {
          "id": "q24",
          "type": "parsons_problem",
          "question_text": "Arrange adapter dropout.",
          "scrambled_code_blocks": [
            { "id": "b1", "text": "delta = self.dropout(delta)" },
            { "id": "b2", "text": "delta = self.B(self.A(x)) * (self.alpha / self.r)" },
            { "id": "b3", "text": "return base + delta" },
            { "id": "b4", "text": "base = F.linear(x, self.weight, self.bias)" },
            { "id": "b5", "text": "def forward(self, x):" }
          ],
          "correct_order": ["b5", "b4", "b2", "b1", "b3"],
          "explanation": "Dropout on adapter delta."
        },
        {
          "id": "q25",
          "type": "parsons_problem",
          "question_text": "Arrange to scale LoRA by timestep embedding.",
          "scrambled_code_blocks": [
            { "id": "b1", "text": "scale = mlp(t_emb)[:, :, None]" },
            { "id": "b2", "text": "delta = delta * scale" },
            { "id": "b3", "text": "return base + delta" },
            { "id": "b4", "text": "def forward(self, x, t_emb):" },
            { "id": "b5", "text": "base = F.linear(x, self.weight, self.bias)" },
            { "id": "b6", "text": "delta = self.B(self.A(x)) * (self.alpha / self.r)" }
          ],
          "correct_order": ["b4", "b5", "b6", "b1", "b2", "b3"],
          "explanation": "FiLM-like scaling of LoRA delta by timestep."
        },
        {
          "id": "q26",
          "type": "parsons_problem",
          "question_text": "Arrange to merge LoRA into base weight.",
          "scrambled_code_blocks": [
            { "id": "b1", "text": "merged = weight + (alpha/r) * (A @ B)" },
            { "id": "b2", "text": "return merged" },
            { "id": "b3", "text": "def merge_lora(weight, A, B, alpha, r):" }
          ],
          "correct_order": ["b3", "b1", "b2"],
          "explanation": "Merge low-rank update into weight."
        },
        {
          "id": "q27",
          "type": "parsons_problem",
          "question_text": "Arrange to disable LoRA (inference without adapters).",
          "scrambled_code_blocks": [
            { "id": "b1", "text": "self.active = False" },
            { "id": "b2", "text": "def disable(self):" },
            { "id": "b3", "text": "return self" }
          ],
          "correct_order": ["b2", "b1", "b3"],
          "explanation": "Flag to bypass delta."
        },
        {
          "id": "q28",
          "type": "parsons_problem",
          "question_text": "Arrange to ensure only LoRA params require grads.",
          "scrambled_code_blocks": [
            { "id": "b1", "text": "for n,p in model.named_parameters():" },
            { "id": "b2", "text": "    p.requires_grad = ('lora' in n)" },
            { "id": "b3", "text": "def grads_for_lora(model):" },
            { "id": "b4", "text": "return model" }
          ],
          "correct_order": ["b3", "b1", "b2", "b4"],
          "explanation": "Gate grads to LoRA modules."
        },
        {
          "id": "q29",
          "type": "parsons_problem",
          "question_text": "Arrange to compute LoRA parameter count.",
          "scrambled_code_blocks": [
            { "id": "b1", "text": "params = sum(p.numel() for n,p in model.named_parameters() if 'lora' in n)" },
            { "id": "b2", "text": "return params" },
            { "id": "b3", "text": "def lora_params(model):" }
          ],
          "correct_order": ["b3", "b1", "b2"],
          "explanation": "Count LoRA parameters."
        },
        {
          "id": "q30",
          "type": "parsons_problem",
          "question_text": "Arrange to apply adapter dropout only during training.",
          "scrambled_code_blocks": [
            { "id": "b1", "text": "if self.training: delta = self.dropout(delta)" },
            { "id": "b2", "text": "return base + delta" },
            { "id": "b3", "text": "delta = self.B(self.A(x)) * (self.alpha / self.r)" },
            { "id": "b4", "text": "def forward(self, x):" },
            { "id": "b5", "text": "base = F.linear(x, self.weight, self.bias)" }
          ],
          "correct_order": ["b4", "b5", "b3", "b1", "b2"],
          "explanation": "Condition dropout on training flag."
        },
        {
          "id": "q31",
          "type": "fill_in_blank",
          "question_text": "LoRA update delta = (alpha / r) * ______ * B.",
          "correct_answer": "A",
          "explanation": "Delta is product of A and B scaled."
        },
        {
          "id": "q32",
          "type": "fill_in_blank",
          "question_text": "Higher rank r increases adapter ______.",
          "correct_answer": "capacity/parameters",
          "explanation": "More rank = more capacity."
        },
        {
          "id": "q33",
          "type": "fill_in_blank",
          "question_text": "Zero-init keeps model identical to ______ at start.",
          "correct_answer": "base",
          "explanation": "No change before training."
        },
        {
          "id": "q34",
          "type": "fill_in_blank",
          "question_text": "Temporal LoRA should wrap ______ attention projections.",
          "correct_answer": "temporal",
          "explanation": "Target temporal Q/K/V/O."
        },
        {
          "id": "q35",
          "type": "fill_in_blank",
          "question_text": "Adapter dropout regularizes ______ outputs.",
          "correct_answer": "LoRA",
          "explanation": "Drop delta."
        },
        {
          "id": "q36",
          "type": "fill_in_blank",
          "question_text": "Sharing LoRA across layers reduces ______ but may underfit.",
          "correct_answer": "parameters/capacity",
          "explanation": "Shared adapters trade capacity."
        },
        {
          "id": "q37",
          "type": "fill_in_blank",
          "question_text": "Merging LoRA is ______ if you discard adapters.",
          "correct_answer": "destructive",
          "explanation": "Cannot unmerge after discarding adapters."
        },
        {
          "id": "q38",
          "type": "fill_in_blank",
          "question_text": "LoRA on temporal modules without T>1 leads to learning only ______ cues.",
          "correct_answer": "spatial",
          "explanation": "No temporal signal at T=1."
        },
        {
          "id": "q39",
          "type": "fill_in_blank",
          "question_text": "Per-head LoRA increases head-specific ______.",
          "correct_answer": "expressiveness",
          "explanation": "More nuanced head updates."
        },
        {
          "id": "q40",
          "type": "fill_in_blank",
          "question_text": "Scaling LoRA by timestep is a ______-style modulation.",
          "correct_answer": "FiLM",
          "explanation": "FiLM uses scale/shift from conditioning."
        }
      ]
    },
    {
      "section_id": "C",
      "section_title": "Deep Theory & Coding",
      "total_questions": 10,
      "marking_scheme": { "correct": 4.0, "incorrect": -1.0 },
      "questions": [
        {
          "id": "q41",
          "type": "theory_open_ended",
          "question_text": "Design a LoRA strategy for WAN 2.2 temporal attention: which projections to wrap, rank choices per scale, and alpha settings. Justify parameter budget vs motion fidelity.",
          "ai_grading_rubric": {
            "key_points_required": [
              "Identify Q/K/V/O in temporal layers",
              "Rank selection higher at mid/high res for detail",
              "Alpha tuning per layer",
              "Trade-offs with VRAM and fidelity"
            ],
            "max_score": 4,
            "min_score": -1
          }
        },
        {
          "id": "q42",
          "type": "theory_open_ended",
          "question_text": "Explain per-head vs fused LoRA for temporal attention and when per-head is worth the cost.",
          "ai_grading_rubric": {
            "key_points_required": [
              "Per-head adds capacity; fused cheaper",
              "Per-head useful for diverse motion patterns",
              "Cost implications in 5B-scale models",
              "Empirical signs of head specialization"
            ],
            "max_score": 4,
            "min_score": -1
          }
        },
        {
          "id": "q43",
          "type": "theory_open_ended",
          "question_text": "Propose a timestep-aware LoRA scaling function and argue why temporal cues might need stronger weighting at high noise.",
          "ai_grading_rubric": {
            "key_points_required": [
              "Function of log-SNR or t",
              "High noise needs structure from temporal cues",
              "Lower noise steps may need less modulation",
              "Implementation: FiLM from t embedding"
            ],
            "max_score": 4,
            "min_score": -1
          }
        },
        {
          "id": "q44",
          "type": "theory_open_ended",
          "question_text": "Describe how to apply LoRA to conv3d layers and compare to attention-only LoRA for temporal adaptation.",
          "ai_grading_rubric": {
            "key_points_required": [
              "Flatten conv weights to linear, apply low-rank update",
              "Conv LoRA influences local motion features",
              "Attention LoRA captures long-range dependencies",
              "Trade-offs in capacity and cost"
            ],
            "max_score": 4,
            "min_score": -1
          }
        },
        {
          "id": "q45",
          "type": "theory_open_ended",
          "question_text": "Discuss risks of overfitting temporal LoRA and regularization techniques (adapter dropout, weight decay, lower alpha/r).",
          "ai_grading_rubric": {
            "key_points_required": [
              "Overfitting yields brittle motion, loss drop but poor generalization",
              "Adapter dropout and lower rank/alpha reduce overfit",
              "Weight decay on adapters",
              "Validation strategies to detect overfit"
            ],
            "max_score": 4,
            "min_score": -1
          }
        },
        {
          "id": "q46",
          "type": "theory_open_ended",
          "question_text": "Explain why merging LoRA is destructive and how to manage multiple LoRA variants for different motion styles.",
          "ai_grading_rubric": {
            "key_points_required": [
              "Merging adds delta to base; unmerge impossible without delta",
              "Keep adapters separate for swapability",
              "Runtime composition strategies",
              "Versioning and storage trade-offs"
            ],
            "max_score": 4,
            "min_score": -1
          }
        },
        {
          "id": "q47",
          "type": "theory_open_ended",
          "question_text": "Construct an experiment to measure sensitivity to LoRA rank on temporal coherence and VRAM for a fixed WAN setup.",
          "ai_grading_rubric": {
            "key_points_required": [
              "Vary ranks, keep data/steps fixed",
              "Measure VRAM, throughput, temporal metrics (warp error/tLPIPS)",
              "Analyze diminishing returns",
              "Report qualitative outcomes"
            ],
            "max_score": 4,
            "min_score": -1
          }
        },
        {
          "id": "q48",
          "type": "theory_open_ended",
          "question_text": "Detail how to safely freeze non-LoRA parameters and verify no gradient leakage in optimizer/state.",
          "ai_grading_rubric": {
            "key_points_required": [
              "Set requires_grad, filter optimizer params",
              "Check grads zero for frozen params",
              "Optimizer state excludes frozen params",
              "Unit tests for accidental updates"
            ],
            "max_score": 4,
            "min_score": -1
          }
        },
        {
          "id": "q49",
          "type": "theory_open_ended",
          "question_text": "Compare using a single shared temporal LoRA vs separate LoRA per resolution. When does sharing break down?",
          "ai_grading_rubric": {
            "key_points_required": [
              "Shared reduces params; assumes similar dynamics across scales",
              "Breakdown when motion patterns differ by scale",
              "Per-scale adapters capture scale-specific motion",
              "Empirical signs of underfitting with shared"
            ],
            "max_score": 4,
            "min_score": -1
          }
        },
        {
          "id": "q50",
          "type": "theory_open_ended",
          "question_text": "Explain how to integrate LoRA with CFG in temporal attention and why guidance strength should respect adapter scale.",
          "ai_grading_rubric": {
            "key_points_required": [
              "LoRA modifies conditional/unconditional branches equally",
              "CFG scales difference; large w can overshoot if adapter strong",
              "Balance alpha/r with guidance",
              "Validation under varying w"
            ],
            "max_score": 4,
            "min_score": -1
          }
        }
      ]
    }
  ]
}
